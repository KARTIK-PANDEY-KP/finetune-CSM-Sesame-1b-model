# CSM-LoRA: Parameter-Efficient Fine-Tuning for Text-to-Speech Models

This repository contains code for fine-tuning CSM (Conversational Speech Model) is a speech generation model from Sesame using LoRA (Low-Rank Adaptation), a parameter-efficient method that allows fine-tuning large models with minimal memory requirements.

## Installation

```bash
# Clone the repository
git clone https://github.com/KARTIK-PANDEY-KP/finetune-CSM-Sesame-1b-model.git
cd csm-lora

# Install dependencies
pip install -r requirements.txt
```

## Hardware Configuration Options

The training script supports various hardware configurations:

### High-End GPUs (A100, H100, etc.)

For machines with powerful GPUs and ample memory:

```bash
python train_lora.py \
  --batch_size 8 \
  --gradient_accumulation_steps 1 \
  --learning_rate 2e-4 \
  --use_wandb True \
  --use_cpu_fallback False
```

### Mid-Range GPUs (RTX 3090, RTX 4090, etc.)

For consumer-grade GPUs with 24-32GB VRAM:

```bash
python train_lora.py \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --learning_rate 1e-4 \
  --use_wandb True \
  --use_cpu_fallback False
```

### Low-End GPUs (GTX 1080, RTX 2060, etc.)

For GPUs with limited VRAM (8-12GB):

```bash
python train_lora.py \
  --batch_size 1 \
  --gradient_accumulation_steps 16 \
  --learning_rate 5e-5 \
  --use_wandb False \
  --use_cpu_fallback False
```

### CPU-Only Training

For testing or when no GPU is available:

```bash
python train_lora.py \
  --batch_size 1 \
  --gradient_accumulation_steps 32 \
  --learning_rate 5e-5 \
  --use_wandb False \
  --use_cpu_fallback True
```

## Key Configuration Parameters

You can modify these parameters in the `train_config` dictionary within `train_lora.py`:

| Parameter | Description | Recommendation |
|-----------|-------------|----------------|
| `batch_size` | Number of samples per batch | High-end GPU: 4-8, Mid-range: 2-4, Low-end: 1-2, CPU: 1 |
| `gradient_accumulation_steps` | Number of steps to accumulate gradients before update | Inversely proportional to batch size |
| `learning_rate` | Learning rate for optimizer | 1e-4 to 5e-5 (lower for smaller batches) |
| `num_epochs` | Number of training epochs | 3-10 depending on dataset size |
| `lora_r` | LoRA rank dimension | 8-32 (higher = more capacity, more memory) |
| `lora_alpha` | LoRA alpha scaling factor | Usually 2x `lora_r` |
| `lora_dropout` | Dropout rate for LoRA layers | 0.05-0.1 |
| `use_cpu_fallback` | Use CPU when OOM errors occur | `True` for testing, `False` for production |
| `max_grad_norm` | Maximum gradient norm for clipping | 1.0 (maybe lower for stability) |

## Memory Optimization Tips

1. **Reduce batch size**: This is the most effective way to reduce memory usage
2. **Increase gradient accumulation**: Compensates for smaller batch sizes
3. **Reduce `lora_r`**: Lower rank means fewer parameters to train
4. **Use FP16/BF16**: Enable mixed precision for faster training and lower memory (not included in the current code)
5. **Offload to CPU**: Using `use_cpu_fallback=True` offloads some operations to CPU

## Dataset Preparation

### Using prepare_dataset.py

The repository includes a `prepare_dataset.py` script that automatically prepares the LJSpeech dataset for training. Here's how to use it:

1. **Setup Environment**:
   Make sure you have all dependencies installed and enough disk space (at least 10GB recommended).

2. **Run the Script**:
   ```bash
   python prepare_dataset.py
   ```

   This will:
   - Download the LJSpeech dataset automatically
   - Create train/validation splits (90%/10%)
   - Process audio files and generate tokens
   - Save the processed data in JSON format

3. **Output Structure**:
   The script creates a `dataset` directory with the following structure:
   ```
   dataset/
   ├── train/
   │   └── data.json
   └── val/
       └── data.json
   ```

4. **Data Format**:
   Each JSON file contains a list of entries with:
   - `text_tokens`: Tokenized text using Llama-3 tokenizer
   - `audio_tokens`: Audio tokens generated by CSM's audio tokenizer
   - `text`: Original text for reference
   - `speaker_id`: Speaker identifier (0 for LJSpeech)

### Custom Dataset Preparation

To use your own dataset, prepare it in the following format:

```json
[
  {
    "text_tokens": [1, 2, 3, ...],      // Text tokens from Llama-3 tokenizer
    "audio_tokens": [[1, 2, 3, ...], [4, 5, 6, ...], ...],  // Audio tokens from CSM
    "text": "Original text",             // Optional: original text
    "speaker_id": 0                      // Optional: speaker identifier
  },
  ...
]
```

Requirements for custom datasets:
1. Audio files should be single-channel (mono)
2. Sample rate will be automatically converted to model's required rate
3. Text should be preprocessed (normalized, cleaned)
4. Audio tokens can be flat or nested lists

### Memory Management

The script includes memory optimization features:
- Processes audio files in batches
- Saves intermediate results every 1000 samples
- Automatically handles GPU/CPU processing
- Cleans up temporary files

### Troubleshooting Dataset Preparation

1. **Out of Memory During Preparation**:
   - Process smaller batches by modifying the save frequency
   - Use CPU processing if GPU memory is limited

2. **Audio Processing Issues**:
   - Check audio file format (WAV recommended)
   - Ensure audio files are not corrupted
   - Verify sample rate conversion settings

3. **Token Generation Errors**:
   - Verify text preprocessing
   - Check tokenizer compatibility
   - Ensure audio segments are within acceptable length

4. **File Permission Issues**:
   - Check write permissions in the dataset directory
   - Verify disk space availability

## Monitoring and Checkpoints

- Training progress is displayed with a progress bar
- Model checkpoints are saved after each epoch in the `checkpoints/` directory
- If enabled, training metrics are logged to wandb

## Example: Editing Configuration

To modify the training configuration, edit the `train_config` dictionary in `train_lora.py`:

```python
train_config = {
    "batch_size": 2,               # Adjust based on your GPU
    "learning_rate": 1e-4,
    "num_epochs": 5,
    "warmup_steps": 100,
    "gradient_accumulation_steps": 8,
    "max_grad_norm": 1.0,
    "lora_r": 16,                  # Increase for more capacity
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "use_wandb": True,             # Set to False if not using wandb
    "checkpoint_dir": "checkpoints",
    "use_cpu_fallback": False      # Set to True for CPU training
}
```

## Troubleshooting

1. **CUDA Out of Memory error**: Try reducing batch size or increasing gradient accumulation
2. **Slow training on CPU**: This is expected; use CPU mode only for testing
3. **Loss not decreasing**: Try adjusting learning rate or check dataset quality
4. **Validation errors**: Make sure vocabulary size matches your dataset

For more information, check the documentation or submit an issue on GitHub.
